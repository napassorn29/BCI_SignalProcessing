{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mne\n",
    "import os\n",
    "import pyedflib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from mne.io import read_raw_edf, RawArray, concatenate_raws\n",
    "from mne.stats import permutation_cluster_1samp_test as pcluster_test\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import autoreject\n",
    "from autoreject import get_rejection_threshold\n",
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import pywt\n",
    "from scipy.stats import pointbiserialr\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D,MaxPooling2D, Flatten\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import mrmr\n",
    "from mrmr import mrmr_classif\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(subject_id:list, task_id:list, montage_name:str):\n",
    "    dataset_path = \"Online Dataset/eeg-motor-movementimagery-dataset-1.0.0/files\"\n",
    "\n",
    "    # --- Full Path ---\n",
    "    filenames = []\n",
    "    for i in range(len(subject_id)):\n",
    "        for j in range(len(task_id)):\n",
    "            filenames.append(\"S\"+subject_id[i]+\"/S\"+subject_id[i]+\"R\"+task_id[j]+\".edf\")\n",
    "            \n",
    "    path = [os.path.join(dataset_path, filename).replace(\"\\\\\", \"/\") for filename in filenames]\n",
    "\n",
    "    # --- Read EDF Files ---\n",
    "    subject_raws = []\n",
    "    for file_path in path:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "            data = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n",
    "        subject_raws.append(data)\n",
    "\n",
    "    # print(\"Path:\",path)\n",
    "    raws_data = concatenate_raws(subject_raws)\n",
    "    # print(\"RawEDF data:\", raws_data)\n",
    "\n",
    "    # --- Check Signal Channels ---\n",
    "    with pyedflib.EdfReader(path[-1]) as edf_file:\n",
    "        signal_labels = edf_file.getSignalLabels()\n",
    "        # print(\"signal_channels:\", signal_labels)\n",
    "    \n",
    "    with open(\"Online Dataset/eeg-motor-movementimagery-dataset-1.0.0/files/wfdbcal\", \"r\") as file:\n",
    "        content = file.readlines()\n",
    "    \n",
    "    print(subject_raws)\n",
    "    \n",
    "    chan_name = []\n",
    "    chan_order = []\n",
    "    chan_mapping = {}\n",
    "    order = 1\n",
    "    for line in content:\n",
    "        parts = line.split('\\t')\n",
    "        channel_name = parts[0].strip()\n",
    "        channel_name = channel_name.replace(\".\", \"\")\n",
    "        chan_name.append(channel_name)\n",
    "        order_name = \"# \" + str(order)\n",
    "        chan_order.append(order_name)\n",
    "        chan_mapping[order_name] = channel_name\n",
    "        order += 1 \n",
    "\n",
    "    channel_names = [chan_mapping[f'# {i+1}'] for i in range(64)]\n",
    "    old_ch_names = raws_data.info['ch_names']\n",
    "\n",
    "    raws_data.rename_channels({old: new for old, new in zip(old_ch_names, channel_names)})\n",
    "\n",
    "    # Set montage\n",
    "    # montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    raws_data.set_montage(montage = mne.channels.make_standard_montage(montage_name))\n",
    "\n",
    "    # Plot channel locations\n",
    "    # raws_data.plot_sensors(show_names=True);\n",
    "    \n",
    "    # raws_data.compute_psd().plot_topomap();\n",
    "    # raws_data.compute_psd().plot();\n",
    "    \n",
    "    return raws_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(raws_data, event, chans_selected:list):\n",
    "    # --- Apply Re-reference by Common Average Reference (CAR) ---\n",
    "    streams = raws_data.copy().set_eeg_reference('average', projection=True)\n",
    "    streams.apply_proj()\n",
    "    \n",
    "    # --- Filter Data ---\n",
    "    stream_filter = streams.copy().filter(l_freq=8.0, h_freq=15.0, method = 'iir', iir_params= {\"order\": 6, \"ftype\":'butter'})\n",
    "    \n",
    "    # --- Apply ICA ---\n",
    "    ica = mne.preprocessing.ICA(n_components=63, random_state=97, max_iter=800)\n",
    "    ica.fit(stream_filter.copy())\n",
    "    \n",
    "    # --- Find Bad Components ---\n",
    "    bad_idx, scores = ica.find_bads_eog(stream_filter.copy(), ch_name='T9', threshold=1.5)\n",
    "    \n",
    "    # --- Remove Bad Components ---\n",
    "    ica.exclude = bad_idx\n",
    "    \n",
    "    # --- Apply ICA to Filtered Data ---\n",
    "    stream_ica = ica.apply(stream_filter.copy(),exclude=ica.exclude)\n",
    "    \n",
    "    # --- Events ---\n",
    "    events, event_dict = mne.events_from_annotations(stream_filter)\n",
    "    \n",
    "    # --- Epoch ---\n",
    "    epochs = mne.Epochs(stream_ica.copy().filter(l_freq=8, h_freq=15.0, method = 'iir', iir_params= {\"order\": 6, \"ftype\":'butter'}), events, tmin = -0.5, tmax = 4, \n",
    "                    event_id = event, preload= True, verbose=False, event_repeated='drop')\n",
    "    \n",
    "    # --- Selected channels are interested ---\n",
    "    epochs = epochs.pick_channels(chans_selected)\n",
    "    \n",
    "    # --- Baseline Correction ---\n",
    "    Baseline = epochs.copy().filter(l_freq=8.0, h_freq=15.0, method = 'iir', iir_params= {\"order\": 6, \"ftype\":'butter'})\n",
    "    stream_mi = Baseline.copy().apply_baseline((-0.5, 0))\n",
    "    \n",
    "    # --- reject bad channels ---\n",
    "    def autoreject_epochs(epochs):\n",
    "        reject = get_rejection_threshold(epochs)  \n",
    "        reject.update(reject)\n",
    "        epochs.drop_bad(reject = reject)\n",
    "        return epochs\n",
    "    stream_mi = autoreject_epochs(stream_mi.copy())\n",
    "    \n",
    "    return stream_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_CSP(epochs, n_component = 7):\n",
    "    X = epochs.get_data()\n",
    "    y = epochs.events[:, -1]\n",
    "    \n",
    "    # Initilize CSP\n",
    "    csp = CSP(n_components = n_component, norm_trace=False)\n",
    "    csp_wt = CSP(n_components = n_component, reg=None, log=None, norm_trace=False, transform_into='csp_space')\n",
    "    \n",
    "    # Fit CSP to data \n",
    "    csp.fit(X, y)\n",
    "    csp_wt.fit(X, y)\n",
    "    \n",
    "    new_data = csp_wt.transform(X)\n",
    "    \n",
    "    # Visualize CSP patterns\n",
    "    csp.plot_patterns(epochs.info);\n",
    "    print(csp)\n",
    "    return X, y, csp, csp_wt, new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_WT(epochs, target , n_component = 7):\n",
    "    epochs_data = epochs.get_data()\n",
    "    labels = epochs.events[:,-1]\n",
    "    \n",
    "    train_size = len(labels)\n",
    "    train_data_cwt = np.ndarray(shape=(train_size ,epochs_data.shape[1] , 8, epochs_data.shape[2],))\n",
    "    \n",
    "    scales = range(8,16)\n",
    "\n",
    "    X_mi, y_mi, csp, csp_wt, data_csp = extract_CSP(epochs.copy(), n_component)\n",
    "    \n",
    "    for ii in range(train_size):\n",
    "        for jj in range(n_component):\n",
    "            signal = data_csp[ii, jj, :]\n",
    "            coeff, _ = pywt.cwt(signal, scales, 'morl', 1)\n",
    "            coeff_ = coeff[:, :epochs_data.shape[2]]  # (8,epochs_data.shape[2])\n",
    "            train_data_cwt[ii, jj, :, :] = np.abs(coeff_)  \n",
    "\n",
    "    for j in range(4):\n",
    "        sample = train_data_cwt[j]\n",
    "        # Create subplots for each channel with a larger figure size\n",
    "        fig, axs = plt.subplots(1, n_component, figsize=(40, 5))  # Increase the width (20) to make the figure larger\n",
    "        for i in range(n_component):\n",
    "            axs[i].imshow(sample[i], aspect='auto', cmap='viridis')\n",
    "            axs[i].set_title(f'Channel {i+1}')\n",
    "        fig.suptitle(f'Class {labels[j]}', fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    wt_shape = train_data_cwt.shape\n",
    "    data_WT = np.reshape(train_data_cwt,(wt_shape[0],wt_shape[1]*wt_shape[2]*wt_shape[3]))\n",
    "    \n",
    "    return train_data_cwt, data_WT, labels, y_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Details --- \n",
    "subject_id = [\"001\"]\n",
    "task_id = [\"04\", \"08\", \"12\"]\n",
    "\n",
    "# --- Set Montage ---\n",
    "montage_name = 'standard_1020'\n",
    "\n",
    "# --- Set Channels Select ---\n",
    "chans_selected = ['Fc5', 'Fc3', 'Fc1', 'Fcz', 'Fc2', 'Fc4', 'Fc6', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'Cp5', 'Cp3', 'Cp1', 'Cpz', 'Cp2', 'Cp4', 'Cp6', 'Fp1', 'Fpz', 'Fp2', 'Af7', 'Af3', 'Afz', 'Af4', 'Af8', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'Ft7', 'Ft8', 'T7', 'T8', 'Tp7', 'Tp8', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'Po7', 'Po3', 'Poz', 'Po4', 'Po8', 'O1', 'Oz', 'O2', 'Iz']\n",
    "\n",
    "#['C5','C3','C1','Cz','C2','C4','C6']\n",
    "\n",
    "# --- Set event ---\n",
    "# event = {'rest':1, 'left': 2, 'right': 3}\n",
    "# target = ['rest','left','right']\n",
    "# numclass = [1,2,3]\n",
    "\n",
    "event = {'left': 2, 'right': 3}\n",
    "target = ['left','right']\n",
    "numclass = [2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(chans_selected).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<RawEDF | S001R04.edf, 64 x 60000 (375.0 s), ~29.4 MB, data loaded>, <RawEDF | S001R08.edf, 64 x 20000 (125.0 s), ~9.8 MB, data loaded>, <RawEDF | S001R12.edf, 64 x 20000 (125.0 s), ~9.8 MB, data loaded>]\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Filtering raw data in 3 contiguous segments\n",
      "Setting up band-pass filter from 8 - 15 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 24 (effective, after forward-backward)\n",
      "- Cutoffs at 8.00, 15.00 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Fitting ICA to data using 64 channels (please be patient, this may take a while)\n",
      "    Applying projection operator with 1 vector (pre-whitener computation)\n",
      "    Applying projection operator with 1 vector (pre-whitener application)\n",
      "Selecting by number: 63 components\n",
      "    Applying projection operator with 1 vector (pre-whitener application)\n",
      "Fitting ICA took 12.7s.\n",
      "Using EOG channel: T9\n",
      "    Applying projection operator with 1 vector (pre-whitener application)\n",
      "... filtering ICA sources\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 1600 samples (10.000 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... filtering target\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 1600 samples (10.000 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying ICA to Raw instance\n",
      "    Applying projection operator with 1 vector (pre-whitener application)\n",
      "    Transforming to ICA space (63 components)\n",
      "    Zeroing out 14 ICA components\n",
      "    Projecting back using 64 PCA components\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "Filtering raw data in 3 contiguous segments\n",
      "Setting up band-pass filter from 8 - 15 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 24 (effective, after forward-backward)\n",
      "- Cutoffs at 8.00, 15.00 Hz: -6.02, -6.02 dB\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Setting up band-pass filter from 8 - 15 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 24 (effective, after forward-backward)\n",
      "- Cutoffs at 8.00, 15.00 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Applying baseline correction (mode: mean)\n",
      "Estimating rejection dictionary for eeg\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "raws_data = load_data(subject_id, task_id, montage_name)\n",
    "epochs = preprocessing(raws_data, event, chans_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = epochs.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 34, 721, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = np.expand_dims(np.moveaxis(data, 0, 0), -1)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "y = epochs.events[:, -1]\n",
    "y = encoder.fit_transform(np.array(y).reshape(-1, 1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_X, y, test_size=0.35, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29, 34, 721, 1), (16, 34, 721, 1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29, 2), (16, 2))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSNN(tf.keras.Model):\n",
    "    tf.keras.backend.set_floatx(\"float64\")\n",
    "    def __init__(self):\n",
    "        super(MSNN, self).__init__()\n",
    "        self.C = 34 # the number of electrodes\n",
    "        self.fs = 160 # the sampling frequency\n",
    "\n",
    "        # Regularizer\n",
    "        self.regularizer = tf.keras.regularizers.L1L2(l1=.001, l2=.01)\n",
    "\n",
    "        # Activation functions\n",
    "        self.activation = tf.keras.layers.LeakyReLU()\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "        \n",
    "        # Define convolutions\n",
    "        conv = lambda D, kernel : tf.keras.layers.Conv2D(D, kernel, kernel_regularizer=self.regularizer)\n",
    "        sepconv = lambda D, kernel : tf.keras.layers.SeparableConv2D(D, kernel, padding=\"same\",\n",
    "                                                                    depthwise_regularizer=self.regularizer,\n",
    "                                                                    pointwise_regularizer=self.regularizer)\n",
    "        \n",
    "        # Spectral convoltuion\n",
    "        self.conv0 = conv(4, (1, int(self.fs/2)))\n",
    "        \n",
    "        # Spatio-temporal convolution\n",
    "        self.conv1t = sepconv(16, (1, 25))\n",
    "        self.conv1s = conv(16, (self.C, 1))\n",
    "        \n",
    "        self.conv2t = sepconv(32, (1, 15))\n",
    "        self.conv2s = conv(32, (self.C, 1))\n",
    "        \n",
    "        self.conv3t = sepconv(64, (1, 6))\n",
    "        self.conv3s = conv(64, (self.C, 1))\n",
    "\n",
    "        # Flatteninig\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "        # Decision making\n",
    "        self.dense = tf.keras.layers.Dense(2, activation=None, kernel_regularizer=self.regularizer)\n",
    "\n",
    "    def embedding(self, x, random_mask=False):\n",
    "        x = self.activation(self.conv0(x))\n",
    "\n",
    "        x = self.activation(self.conv1t(x))\n",
    "        f1 = self.activation(self.conv1s(x))\n",
    "\n",
    "        x = self.activation(self.conv2t(x))\n",
    "        f2 = self.activation(self.conv2s(x))\n",
    "\n",
    "        x = self.activation(self.conv3t(x))\n",
    "        f3 = self.activation(self.conv3s(x))\n",
    "\n",
    "        feature = tf.concat((f1, f2, f3), -1)\n",
    "        return feature\n",
    "\n",
    "    def classifier(self, feature):\n",
    "        # Flattening, dropout, mapping into the decision nodes\n",
    "        feature = self.flatten(feature)\n",
    "        feature = self.dropout(feature)\n",
    "        y_hat = self.softmax(self.dense(feature))\n",
    "        return y_hat\n",
    "\n",
    "    def GAP(self, feature):\n",
    "        return tf.reduce_mean(feature, -2)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Extract feature using MSNN encoder\n",
    "        feature = self.embedding(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        feature = self.GAP(feature)\n",
    "\n",
    "        # Decision making\n",
    "        y_hat = self.classifier(feature)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shallow_convnet(tf.keras.Model):\n",
    "    tf.keras.backend.set_floatx(\"float64\")\n",
    "    def __init__(self):\n",
    "        super(Shallow_convnet, self).__init__()\n",
    "        self.C = 34 # the number of electrodes\n",
    "        self.fs = 160 # the sampling frequency\n",
    "\n",
    "        # Regularizer\n",
    "        self.regularizer = tf.keras.regularizers.L1L2(l1=.001, l2=.01)\n",
    "\n",
    "        # Activation functions\n",
    "        self.activation = tf.keras.layers.LeakyReLU()\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "        \n",
    "        # Define convolutions\n",
    "        conv = lambda D, kernel : tf.keras.layers.Conv2D(D, kernel, kernel_regularizer=self.regularizer)\n",
    "        sepconv = lambda D, kernel : tf.keras.layers.SeparableConv2D(D, kernel, padding=\"same\",\n",
    "                                                                    depthwise_regularizer=self.regularizer,\n",
    "                                                                    pointwise_regularizer=self.regularizer)\n",
    "        pool = lambda p_size : tf.keras.layers.MaxPool2D(p_size, strides=None, padding='valid',\n",
    "                                                 data_format=None, name=None)\n",
    "        \n",
    "        # Spectral convoltuion\n",
    "        self.conv0 = conv(16, (1, int(self.fs/2)))\n",
    "        \n",
    "        # Spatio-temporal convolution\n",
    "        # self.conv1t = sepconv(16, (1, 25))\n",
    "        self.conv1s = conv(16, (self.C, 1))\n",
    "        \n",
    "        # pooling layer\n",
    "        self.pooling0 = pool((2, 2))\n",
    "        \n",
    "        # Flatteninig\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "        # Decision making\n",
    "        self.dense = tf.keras.layers.Dense(2, activation=None, kernel_regularizer=self.regularizer)\n",
    "\n",
    "    def embedding(self, x, random_mask=False):\n",
    "        x = self.activation(self.conv0(x))\n",
    "\n",
    "        # x = self.activation(self.conv1t(x))\n",
    "        f1 = self.activation(self.conv1s(x))\n",
    "\n",
    "        feature = tf.concat((f1), -1)\n",
    "        return feature\n",
    "\n",
    "    def classifier(self, feature):\n",
    "        # Flattening, dropout, mapping into the decision nodes\n",
    "        feature = self.flatten(feature)\n",
    "        feature = self.dropout(feature)\n",
    "        y_hat = self.softmax(self.dense(feature))\n",
    "        return y_hat\n",
    "\n",
    "    def GAP(self, feature):\n",
    "        return tf.reduce_mean(feature, -2)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Extract feature using MSNN encoder\n",
    "        feature = self.embedding(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        feature = self.GAP(feature)\n",
    "\n",
    "        # Decision making\n",
    "        y_hat = self.classifier(feature)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the informative segments selection agent module.\n",
    "# Define actor network (for categorical actions: selection/rejection)\n",
    "class ACTOR(tf.keras.Model):\n",
    "    def __init__(self, n_actions=2):\n",
    "        super().__init__()\n",
    "        self.actor = tf.keras.layers.Dense(n_actions, activation=None, \n",
    "                                          kernel_regularizer=tf.keras.regularizers.L1L2(l1=.001, l2=.01))        \n",
    "    def call(self, segment):\n",
    "        return self.actor(segment) # Outputs logit vector.\n",
    "    \n",
    "# Define critic network\n",
    "class CRITIC(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.critic = tf.keras.layers.Dense(1, activation=None,\n",
    "                                           kernel_regularizer=tf.keras.regularizers.L1L2(l1=.001, l2=.01))\n",
    "    def call(self, segment):\n",
    "        return tf.keras.activations.sigmoid(self.critic(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions.\n",
    "def gradient(model, inputs, labels, mask=None):\n",
    "    with tf.GradientTape() as tape:\n",
    "        if mask is None:\n",
    "            yhat = model(inputs)\n",
    "        else:\n",
    "            feature = model.GAP(model.embedding(inputs) * mask)\n",
    "            yhat = model.classifier(feature)\n",
    "\n",
    "        loss = tf.keras.losses.binary_crossentropy(labels, yhat)\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    return loss, grad\n",
    "\n",
    "def agent_gradient(model, actor, critic, inputs, feature, labels, state, state_next):\n",
    "    gamma = 0.95 # discount factor\n",
    "    with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "        loss_FM = tf.keras.losses.binary_crossentropy(labels, model(inputs))\n",
    "        loss_AM = tf.keras.losses.binary_crossentropy(labels, model.classifier(feature))\n",
    "\n",
    "        # Reward, r_t\n",
    "        reward = loss_FM - loss_AM\n",
    "        # Advantage, A_t\n",
    "        advantage = reward[:, None] + gamma * critic(state_next) - critic(state)            \n",
    "        # Critic loss\n",
    "        critic_loss = 0.5 * tf.math.square(advantage)            \n",
    "        # Actor loss\n",
    "        actor_loss = -tf.math.log(tf.nn.softmax(actor(state))) * advantage\n",
    "\n",
    "    critic_grad = tape1.gradient(critic_loss, critic.trainable_variables)\n",
    "    # print(\"critic_grad: \", critic_grad)\n",
    "    actor_grad = tape2.gradient(actor_loss, actor.trainable_variables)\n",
    "    return critic_loss, critic_grad, actor_loss, actor_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Training Loss 0.6931\n",
      "Iteration 2, Training Loss 0.6877\n",
      "Iteration 3, Training Loss 0.6751\n",
      "Iteration 4, Training Loss 0.6758\n",
      "Iteration 5, Training Loss 0.6760\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_53476\\3333597451.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m \u001b[1;31m# Define experiment conducting class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;31m# Here, we trained and tested MSNN without the proposed agent module.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_53476\\3333597451.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mxb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batch\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYtr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batch\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;31m# Estimate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;31m# Update the parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_53476\\1618907373.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, inputs, labels, mask)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1109\u001b[0m               output_gradients))\n\u001b[0;32m   1110\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1111\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    587\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m           data_format=data_format),\n\u001b[1;32m--> 591\u001b[1;33m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0m\u001b[0;32m    592\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m           \u001b[0mshape_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1260\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1264\u001b[1;33m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1265\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m       return conv2d_backprop_filter_eager_fallback(\n\u001b[0;32m   1267\u001b[0m           \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define experiment conducting class.\n",
    "# Here, we trained and tested MSNN without the proposed agent module.\n",
    "# msnn\n",
    "class experiment():\n",
    "    def __init__(self, train_X, train_Y, test_X, test_Y):\n",
    "        # Load dataset.\n",
    "        # For simplicity, we just removed validating phase here.\n",
    "        self.Xtr, self.Ytr = train_X, train_Y\n",
    "        self.Xts, self.Yts = test_X, test_Y\n",
    "        self.Yts = np.argmax(self.Yts, axis=-1) # To use scikit-learn accuracy function\n",
    "        \n",
    "        # Randomize the training dataset.\n",
    "        rand_idx = np.random.permutation(self.Xtr.shape[0])\n",
    "        self.Xtr, self.Ytr = self.Xtr[rand_idx, :, :, :], self.Ytr[rand_idx, :]\n",
    "\n",
    "        # Learning schedules\n",
    "        self.init_LR = 1e-3\n",
    "        self.num_epochs_pre = 20 # Pre-training epochs\n",
    "        self.num_epochs = 30\n",
    "        self.num_batch = 20\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.init_LR)\n",
    "        \n",
    "        # Here, we used subject 1's 2nd session data.\n",
    "        # self.sbj_idx, self.sess_idx = 1, 2\n",
    "        # print(f\"START TRAINING Subject {self.sbj_idx}, Session {self.sess_idx}\")\n",
    "        \n",
    "        # Call optimizer.\n",
    "        self.num_batch_iter = int(self.Xtr.shape[0]/self.num_batch)\n",
    "        \n",
    "    def training_FM(self):\n",
    "        # Call MSNN.\n",
    "        msnn = MSNN()\n",
    "        \n",
    "        # To record the loss curve.\n",
    "        loss_FM = []\n",
    "        for epoch in range(self.num_epochs):\n",
    "            loss_per_epoch = 0\n",
    "\n",
    "            for batch in range(self.num_batch_iter):\n",
    "                # Sample minibatch.\n",
    "                xb = self.Xtr[batch * self.num_batch : (batch + 1) * self.num_batch, :, :, :]\n",
    "                yb = self.Ytr[batch * self.num_batch : (batch + 1) * self.num_batch, :]\n",
    "\n",
    "                # Estimate loss\n",
    "                loss, grads = gradient(msnn, xb, yb)\n",
    "\n",
    "                # Update the parameters\n",
    "                self.optimizer.apply_gradients(zip(grads, msnn.trainable_variables))\n",
    "                loss_FM.append(np.mean(loss))\n",
    "                loss_per_epoch += np.mean(loss)\n",
    "\n",
    "            loss_per_epoch /= self.num_batch_iter\n",
    "\n",
    "            # Reporting\n",
    "            print(f\"Iteration {epoch + 1}, Training Loss {loss_per_epoch:>.04f}\")\n",
    "            \n",
    "        # Test the learned model.\n",
    "        Yts_hat = np.argmax(msnn(X_test), axis=-1)\n",
    "        # print(f\"\\nSubject {self.sbj_idx}, Session {self.sess_idx},\\\n",
    "        print(f\"Testing accuracy: {accuracy_score(self.Yts, Yts_hat)}!\\n\")\n",
    "        return loss_FM\n",
    "    \n",
    "    def training_AM(self):\n",
    "        # Call MSNN.\n",
    "        msnn = MSNN()\n",
    "        \n",
    "        # To record the loss curve.\n",
    "        loss_AM = []\n",
    "        # Pre-training without the agent module\n",
    "        for epoch in range(self.num_epochs_pre):\n",
    "            loss_per_epoch = 0\n",
    "\n",
    "            for batch in range(self.num_batch_iter):\n",
    "                # Sample minibatch.\n",
    "                xb = self.Xtr[batch * self.num_batch : (batch + 1) * self.num_batch, :, :, :]\n",
    "                yb = self.Ytr[batch * self.num_batch : (batch + 1) * self.num_batch, :]\n",
    "\n",
    "                # Estimate loss\n",
    "                loss, grads = gradient(msnn, xb, yb)\n",
    "\n",
    "                # Update the parameters\n",
    "                self.optimizer.apply_gradients(zip(grads, msnn.trainable_variables))\n",
    "                loss_AM.append(np.mean(loss))\n",
    "                loss_per_epoch += np.mean(loss)\n",
    "\n",
    "            loss_per_epoch /= self.num_batch_iter\n",
    "\n",
    "            # Reporting\n",
    "            print(f\"Iteration {epoch + 1}, Training Loss {loss_per_epoch:>.04f}\")\n",
    "            \n",
    "        # Call agent module.\n",
    "        actor = ACTOR()\n",
    "        critic = CRITIC()\n",
    "        \n",
    "        # Training with the agent module\n",
    "        for epoch in range(self.num_epochs - self.num_epochs_pre):\n",
    "            loss_per_epoch = 0\n",
    "            \n",
    "            for batch in range(self.num_batch_iter):\n",
    "                # Sample minibatch.\n",
    "                xb = self.Xtr[batch * self.num_batch : (batch + 1) * self.num_batch, :, :, :]\n",
    "                yb = self.Ytr[batch * self.num_batch : (batch + 1) * self.num_batch, :]\n",
    "                # Extract full segments.\n",
    "                features = msnn.embedding(xb)\n",
    "                \n",
    "                agg_wo_current = np.zeros((self.num_batch, features.shape[-1]))\n",
    "                num_added = np.zeros((self.num_batch, features.shape[-1])) # To estimate the denominator.\n",
    "                mask = np.zeros(features.shape) # Mask generated by the agent module\n",
    "                for t in range(features.shape[-2] - 1): # t = 1,...,T'\n",
    "                    print('epoch :',epoch, 'batch :', batch, 't :', t)\n",
    "                    deno1 = np.copy(num_added)\n",
    "                    deno2 = np.copy(num_added) + 1 # For the features with the current segment.\n",
    "                    # To avoid zero-division.\n",
    "                    deno1[deno1 == 0] = 1.\n",
    "                    \n",
    "                    agg_w_current = agg_wo_current + features[:, 0, t, :]\n",
    "                    \n",
    "                    # Define state, s_t.\n",
    "                    state = np.concatenate((agg_wo_current/deno1, agg_w_current/deno2), axis=-1)\n",
    "                    # Get action, a_t.\n",
    "                    action_probs = actor(state)\n",
    "                    action = np.tile(tf.random.categorical(action_probs, 1).numpy(), 112) # (5, 112)\n",
    "                    mask[:, 0, t, :] = action\n",
    "                    num_added += action\n",
    "                    \n",
    "                    # Current feature after action decision, phi_t.\n",
    "                    deno3 = np.copy(num_added)\n",
    "                    deno3[deno3 == 0] = 1 # To avoid zero-division.\n",
    "                    feature = (agg_wo_current + features[:, 0, t, :] * action)/deno3\n",
    "                    \n",
    "                    # Define next state, s_{t+1}, temporally.\n",
    "                    agg_wo_current = feature\n",
    "                    tmp = agg_wo_current + features[:, 0, t + 1, :]\n",
    "                    state_next = np.concatenate((agg_wo_current/deno3, tmp/(deno3 + 1)), axis=-1)\n",
    "\n",
    "                    # Calculate critic and actor loss values\n",
    "                    critic_loss, critic_grads, actor_loss, actor_grads =\\\n",
    "                    agent_gradient(msnn, actor, critic, xb, feature, yb, state, state_next)\n",
    "                    \n",
    "                    # print(actor_grads)\n",
    "                    \n",
    "                    self.optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
    "                    # print(\"critic :\",critic.trainable_variables)\n",
    "                    self.optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
    "                                        \n",
    "                # Finally, predict labels of input EEG using the selected segments.                \n",
    "                # Update the parameters\n",
    "                loss, grads = gradient(msnn, xb, yb, mask)\n",
    "                self.optimizer.apply_gradients(zip(grads, msnn.trainable_variables))\n",
    "                loss_AM.append(np.mean(loss))\n",
    "                loss_per_epoch += np.mean(loss)\n",
    "                \n",
    "            loss_per_epoch /= self.num_batch_iter\n",
    "\n",
    "            # Reporting\n",
    "            print(f\"Iteration {epoch + 1 + self.num_epochs_pre}, Training Loss {loss_per_epoch:>.04f}\")\n",
    "        \n",
    "        # Test the learned model.\n",
    "        Yts_hat = np.argmax(msnn(X_test), axis=-1)\n",
    "        # print(f\"\\nSubject {self.sbj_idx}, Session {self.sess_idx}, \\\n",
    "        print(f\"\\nTesting accuracy: {accuracy_score(self.Yts, Yts_hat)}!\\n\")\n",
    "        return loss_AM\n",
    "        \n",
    "exp = experiment(X_train, y_train, X_test, y_test)\n",
    "loss_FM = exp.training_FM()\n",
    "loss_AM = exp.training_AM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Training Loss 0.6931\n",
      "Iteration 2, Training Loss 0.6918\n",
      "Iteration 3, Training Loss 0.6902\n",
      "Iteration 4, Training Loss 0.6888\n",
      "Iteration 5, Training Loss 0.6882\n",
      "Iteration 6, Training Loss 0.6881\n",
      "Iteration 7, Training Loss 0.6881\n",
      "Iteration 8, Training Loss 0.6881\n",
      "Iteration 9, Training Loss 0.6881\n",
      "Iteration 10, Training Loss 0.6881\n",
      "Iteration 11, Training Loss 0.6881\n",
      "Iteration 12, Training Loss 0.6881\n",
      "Iteration 13, Training Loss 0.6881\n",
      "Iteration 14, Training Loss 0.6881\n",
      "Iteration 15, Training Loss 0.6881\n",
      "Iteration 16, Training Loss 0.6881\n",
      "Iteration 17, Training Loss 0.6881\n",
      "Iteration 18, Training Loss 0.6881\n",
      "Iteration 19, Training Loss 0.6881\n",
      "Iteration 20, Training Loss 0.6881\n",
      "Iteration 21, Training Loss 0.6881\n",
      "Iteration 22, Training Loss 0.6881\n",
      "Iteration 23, Training Loss 0.6881\n",
      "Iteration 24, Training Loss 0.6881\n",
      "Iteration 25, Training Loss 0.6881\n",
      "Iteration 26, Training Loss 0.6881\n",
      "Iteration 27, Training Loss 0.6881\n",
      "Iteration 28, Training Loss 0.6881\n",
      "Iteration 29, Training Loss 0.6881\n",
      "Iteration 30, Training Loss 0.6881\n",
      "Testing accuracy: 0.4375!\n",
      "\n",
      "Iteration 1, Training Loss 0.6931\n",
      "Iteration 2, Training Loss 0.6919\n",
      "Iteration 3, Training Loss 0.6903\n",
      "Iteration 4, Training Loss 0.6889\n",
      "Iteration 5, Training Loss 0.6882\n",
      "Iteration 6, Training Loss 0.6881\n",
      "Iteration 7, Training Loss 0.6881\n",
      "Iteration 8, Training Loss 0.6881\n",
      "Iteration 9, Training Loss 0.6881\n",
      "Iteration 10, Training Loss 0.6881\n",
      "Iteration 11, Training Loss 0.6881\n",
      "Iteration 12, Training Loss 0.6881\n",
      "Iteration 13, Training Loss 0.6881\n",
      "Iteration 14, Training Loss 0.6881\n",
      "Iteration 15, Training Loss 0.6881\n",
      "Iteration 16, Training Loss 0.6881\n",
      "Iteration 17, Training Loss 0.6881\n",
      "Iteration 18, Training Loss 0.6881\n",
      "Iteration 19, Training Loss 0.6881\n",
      "Iteration 20, Training Loss 0.6881\n",
      "epoch : 0 batch : 0 t : 0\n",
      "epoch : 0 batch : 0 t : 1\n",
      "epoch : 0 batch : 0 t : 2\n",
      "epoch : 0 batch : 0 t : 3\n",
      "epoch : 0 batch : 0 t : 4\n",
      "epoch : 0 batch : 0 t : 5\n",
      "epoch : 0 batch : 0 t : 6\n",
      "epoch : 0 batch : 0 t : 7\n",
      "epoch : 0 batch : 0 t : 8\n",
      "epoch : 0 batch : 0 t : 9\n",
      "epoch : 0 batch : 0 t : 10\n",
      "epoch : 0 batch : 0 t : 11\n",
      "epoch : 0 batch : 0 t : 12\n",
      "epoch : 0 batch : 0 t : 13\n",
      "epoch : 0 batch : 0 t : 14\n",
      "epoch : 0 batch : 0 t : 15\n",
      "epoch : 0 batch : 0 t : 16\n",
      "epoch : 0 batch : 0 t : 17\n",
      "epoch : 0 batch : 0 t : 18\n",
      "epoch : 0 batch : 0 t : 19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_53476\\1444542281.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m \u001b[1;31m# Define experiment conducting class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;31m# Here, we trained and tested MSNN without the proposed agent module.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_53476\\1444542281.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m                     \u001b[0mstate_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_wo_current\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdeno3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeno3\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                     \u001b[1;31m# Calculate critic and actor loss values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                     \u001b[0mcritic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_grads\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                     \u001b[0magent_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshallow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[1;31m# print(actor_grads)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_53476\\1618907373.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, actor, critic, inputs, feature, labels, state, state_next)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0madvantage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mcritic_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# print(\"critic_grad: \", critic_grad)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mactor_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1109\u001b[0m               output_gradients))\n\u001b[0;32m   1110\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1111\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    587\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m           data_format=data_format),\n\u001b[1;32m--> 591\u001b[1;33m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0m\u001b[0;32m    592\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m           \u001b[0mshape_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1260\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1264\u001b[1;33m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1265\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m       return conv2d_backprop_filter_eager_fallback(\n\u001b[0;32m   1267\u001b[0m           \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define experiment conducting class.\n",
    "# Here, we trained and tested MSNN without the proposed agent module.\n",
    "# shallow_convnet\n",
    "class experiment():\n",
    "    def __init__(self, train_X, train_Y, test_X, test_Y):\n",
    "        # Load dataset.\n",
    "        # For simplicity, we just removed validating phase here.\n",
    "        self.Xtr, self.Ytr = train_X, train_Y\n",
    "        self.Xts, self.Yts = test_X, test_Y\n",
    "        self.Yts = np.argmax(self.Yts, axis=-1) # To use scikit-learn accuracy function\n",
    "        \n",
    "        # Randomize the training dataset.\n",
    "        rand_idx = np.random.permutation(self.Xtr.shape[0])\n",
    "        self.Xtr, self.Ytr = self.Xtr[rand_idx, :, :, :], self.Ytr[rand_idx, :]\n",
    "\n",
    "        # Learning schedules\n",
    "        self.init_LR = 1e-3\n",
    "        self.num_epochs_pre = 20 # Pre-training epochs\n",
    "        self.num_epochs = 30\n",
    "        self.num_batch = 20\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.init_LR)\n",
    "        \n",
    "        # Here, we used subject 1's 2nd session data.\n",
    "        # self.sbj_idx, self.sess_idx = 1, 2\n",
    "        # print(f\"START TRAINING Subject {self.sbj_idx}, Session {self.sess_idx}\")\n",
    "        \n",
    "        # Call optimizer.\n",
    "        self.num_batch_iter = int(self.Xtr.shape[0]/self.num_batch)\n",
    "        \n",
    "    def training_FM(self):\n",
    "        # Call MSNN.\n",
    "        shallow = Shallow_convnet()\n",
    "        \n",
    "        # To record the loss curve.\n",
    "        loss_FM = []\n",
    "        for epoch in range(self.num_epochs):\n",
    "            loss_per_epoch = 0\n",
    "\n",
    "            for batch in range(self.num_batch_iter):\n",
    "                # Sample minibatch.\n",
    "                xb = self.Xtr[batch * self.num_batch : (batch + 1) * self.num_batch, :, :, :]\n",
    "                yb = self.Ytr[batch * self.num_batch : (batch + 1) * self.num_batch, :]\n",
    "\n",
    "                # Estimate loss\n",
    "                loss, grads = gradient(shallow, xb, yb)\n",
    "\n",
    "                # Update the parameters\n",
    "                self.optimizer.apply_gradients(zip(grads, shallow.trainable_variables))\n",
    "                loss_FM.append(np.mean(loss))\n",
    "                loss_per_epoch += np.mean(loss)\n",
    "\n",
    "            loss_per_epoch /= self.num_batch_iter\n",
    "\n",
    "            # Reporting\n",
    "            print(f\"Iteration {epoch + 1}, Training Loss {loss_per_epoch:>.04f}\")\n",
    "            \n",
    "        # Test the learned model.\n",
    "        Yts_hat = np.argmax(shallow(X_test), axis=-1)\n",
    "        # print(f\"\\nSubject {self.sbj_idx}, Session {self.sess_idx},\\\n",
    "        print(f\"Testing accuracy: {accuracy_score(self.Yts, Yts_hat)}!\\n\")\n",
    "        return loss_FM\n",
    "    \n",
    "    def training_AM(self):\n",
    "        # Call MSNN.\n",
    "        shallow = Shallow_convnet()\n",
    "        \n",
    "        # To record the loss curve.\n",
    "        loss_AM = []\n",
    "        # Pre-training without the agent module\n",
    "        for epoch in range(self.num_epochs_pre):\n",
    "            loss_per_epoch = 0\n",
    "\n",
    "            for batch in range(self.num_batch_iter):\n",
    "                # Sample minibatch.\n",
    "                xb = self.Xtr[batch * self.num_batch : (batch + 1) * self.num_batch, :, :, :]\n",
    "                yb = self.Ytr[batch * self.num_batch : (batch + 1) * self.num_batch, :]\n",
    "\n",
    "                # Estimate loss\n",
    "                loss, grads = gradient(shallow, xb, yb)\n",
    "\n",
    "                # Update the parameters\n",
    "                self.optimizer.apply_gradients(zip(grads, shallow.trainable_variables))\n",
    "                loss_AM.append(np.mean(loss))\n",
    "                loss_per_epoch += np.mean(loss)\n",
    "\n",
    "            loss_per_epoch /= self.num_batch_iter\n",
    "\n",
    "            # Reporting\n",
    "            print(f\"Iteration {epoch + 1}, Training Loss {loss_per_epoch:>.04f}\")\n",
    "            \n",
    "        # Call agent module.\n",
    "        actor = ACTOR()\n",
    "        critic = CRITIC()\n",
    "        \n",
    "        # Training with the agent module\n",
    "        for epoch in range(self.num_epochs - self.num_epochs_pre):\n",
    "            loss_per_epoch = 0\n",
    "            \n",
    "            for batch in range(self.num_batch_iter):\n",
    "                # Sample minibatch.\n",
    "                xb = self.Xtr[batch * self.num_batch : (batch + 1) * self.num_batch, :, :, :]\n",
    "                yb = self.Ytr[batch * self.num_batch : (batch + 1) * self.num_batch, :]\n",
    "                # Extract full segments.\n",
    "                features = shallow.embedding(xb)\n",
    "                \n",
    "                agg_wo_current = np.zeros((self.num_batch, features.shape[-1]))\n",
    "                num_added = np.zeros((self.num_batch, features.shape[-1])) # To estimate the denominator.\n",
    "                mask = np.zeros(features.shape) # Mask generated by the agent module\n",
    "                for t in range(features.shape[-2] - 1): # t = 1,...,T'\n",
    "                    print('epoch :',epoch, 'batch :', batch, 't :', t)\n",
    "                    deno1 = np.copy(num_added)\n",
    "                    deno2 = np.copy(num_added) + 1 # For the features with the current segment.\n",
    "                    # To avoid zero-division.\n",
    "                    deno1[deno1 == 0] = 1.\n",
    "                    \n",
    "                    agg_w_current = agg_wo_current + features[:, 0, t, :]\n",
    "                    \n",
    "                    # Define state, s_t.\n",
    "                    state = np.concatenate((agg_wo_current/deno1, agg_w_current/deno2), axis=-1)\n",
    "                    # Get action, a_t.\n",
    "                    action_probs = actor(state)\n",
    "                    action = np.tile(tf.random.categorical(action_probs, 1).numpy(), 16) # (5, 112)\n",
    "                    mask[:, 0, t, :] = action\n",
    "                    num_added += action\n",
    "                    \n",
    "                    # Current feature after action decision, phi_t.\n",
    "                    deno3 = np.copy(num_added)\n",
    "                    deno3[deno3 == 0] = 1 # To avoid zero-division.\n",
    "                    feature = (agg_wo_current + features[:, 0, t, :] * action)/deno3\n",
    "                    \n",
    "                    # Define next state, s_{t+1}, temporally.\n",
    "                    agg_wo_current = feature\n",
    "                    tmp = agg_wo_current + features[:, 0, t + 1, :]\n",
    "                    state_next = np.concatenate((agg_wo_current/deno3, tmp/(deno3 + 1)), axis=-1)\n",
    "\n",
    "                    # Calculate critic and actor loss values\n",
    "                    critic_loss, critic_grads, actor_loss, actor_grads =\\\n",
    "                    agent_gradient(shallow, actor, critic, xb, feature, yb, state, state_next)\n",
    "                    \n",
    "                    # print(actor_grads)\n",
    "                    \n",
    "                    self.optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
    "                    # print(\"critic :\",critic.trainable_variables)\n",
    "                    self.optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
    "                                        \n",
    "                # Finally, predict labels of input EEG using the selected segments.                \n",
    "                # Update the parameters\n",
    "                loss, grads = gradient(shallow, xb, yb, mask)\n",
    "                self.optimizer.apply_gradients(zip(grads, shallow.trainable_variables))\n",
    "                loss_AM.append(np.mean(loss))\n",
    "                loss_per_epoch += np.mean(loss)\n",
    "                \n",
    "            loss_per_epoch /= self.num_batch_iter\n",
    "\n",
    "            # Reporting\n",
    "            print(f\"Iteration {epoch + 1 + self.num_epochs_pre}, Training Loss {loss_per_epoch:>.04f}\")\n",
    "        \n",
    "        # Test the learned model.\n",
    "        Yts_hat = np.argmax(shallow(X_test), axis=-1)\n",
    "        # print(f\"\\nSubject {self.sbj_idx}, Session {self.sess_idx}, \\\n",
    "        print(f\"\\nTesting accuracy: {accuracy_score(self.Yts, Yts_hat)}!\\n\")\n",
    "        return loss_AM\n",
    "        \n",
    "exp = experiment(X_train, y_train, X_test, y_test)\n",
    "loss_FM = exp.training_FM()\n",
    "loss_AM = exp.training_AM()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
